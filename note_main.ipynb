{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    def __init__(self, verbose = True):\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0,\n",
    "                                            high=4,\n",
    "                                            shape=(4, 4),\n",
    "                                            dtype=np.int16)\n",
    "        self.reward_range = (-200, 200)\n",
    "\n",
    "        self.current_episode = 0\n",
    "        self.success_episode = []\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_player = 1\n",
    "        # P means the game is playable, W means somenone wins, L someone lose\n",
    "        self.state = 'P'\n",
    "        self.current_step = 0\n",
    "        self.max_step = 30\n",
    "        self.world = np.array([[1, 0, 0, 2],\n",
    "                              [0, 0, 0, 0],\n",
    "                              [0, 3, 4, 3],\n",
    "                              [0, 4, 0, 0]])\n",
    "\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.world\n",
    "\n",
    "        obs = np.append(obs, [[self.current_player, 0, 0, 0]], axis=0)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        current_pos = np.where(self.world == self.current_player)\n",
    "\n",
    "        if action == 0:\n",
    "            next_pos = (current_pos[0] - 1, current_pos[1])\n",
    "\n",
    "            if next_pos[0] >= 0 and int(self.world[next_pos]) == 0:\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "\n",
    "            elif next_pos[0] >= 0 and int(self.world[next_pos]) in (1, 2):\n",
    "                pass\n",
    "\n",
    "            elif next_pos[0] >= 0 and (int(self.world[next_pos]) == 3):\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "                self.state = 'L'\n",
    "\n",
    "            elif next_pos[0] >= 0 and (int(self.world[next_pos]) == 4):\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "                self.state = 'W'\n",
    "\n",
    "        elif action == 1:\n",
    "            next_pos = (current_pos[0], current_pos[1] + 1)\n",
    "\n",
    "            if next_pos[1] < 3 and int(self.world[next_pos]) == 0:\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "\n",
    "            elif next_pos[1] < 3 and int(self.world[next_pos]) in (1, 2):\n",
    "                pass\n",
    "\n",
    "            elif next_pos[1] < 3 and (int(self.world[next_pos]) == 3):\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "                self.state = 'L'\n",
    "\n",
    "            elif next_pos[1] < 3 and (int(self.world[next_pos]) == 4):\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "                self.state = 'W'\n",
    "\n",
    "        elif action == 2:\n",
    "            next_pos = (current_pos[0] + 1, current_pos[1])\n",
    "\n",
    "            if next_pos[0] <= 3 and int(self.world[next_pos]) == 0:\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "\n",
    "            elif next_pos[0] <= 3 and int(self.world[next_pos]) in (1, 2):\n",
    "                pass\n",
    "\n",
    "            elif next_pos[0] <= 3 and (int(self.world[next_pos]) == 3):\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "                self.state = 'L'\n",
    "\n",
    "            elif next_pos[0] <= 3 and (int(self.world[next_pos]) == 4):\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "                self.state = 'W'\n",
    "\n",
    "        elif action == 3:\n",
    "            next_pos = (current_pos[0], current_pos[1] - 1)\n",
    "\n",
    "            if next_pos[1] >= 0 and int(self.world[next_pos]) == 0:\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "\n",
    "            elif next_pos[1] >= 0 and int(self.world[next_pos]) in (1, 2):\n",
    "                pass\n",
    "\n",
    "            elif next_pos[1] >= 0 and (int(self.world[next_pos]) == 3):\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "                self.state = 'L'\n",
    "\n",
    "            elif next_pos[1] >= 0 and (int(self.world[next_pos]) == 4):\n",
    "                self.world[next_pos] = self.current_player\n",
    "                self.world[current_pos] = 0\n",
    "                self.state = 'W'\n",
    "\n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        self.current_step += 1\n",
    "        if (self.verbose): print(self.world)\n",
    "\n",
    "        if self.state == \"W\":\n",
    "            if (self.verbose): print(f'Player {self.current_player} won')\n",
    "            reward = 200\n",
    "            done = True\n",
    "        elif self.state == 'L':\n",
    "            if (self.verbose): print(f'Player {self.current_player} lost')\n",
    "            reward = -200\n",
    "            done = True\n",
    "        elif self.state == 'P':\n",
    "            reward = -1\n",
    "            done = False\n",
    "\n",
    "        if self.current_step >= self.max_step:\n",
    "            done = True\n",
    "\n",
    "        if self.current_player == 1:\n",
    "            self.current_player = 2\n",
    "        else:\n",
    "            self.current_player = 1\n",
    "\n",
    "        if done:\n",
    "            self.render_episode(self.state)\n",
    "            self.current_episode += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def render_episode(self, win_or_lose):\n",
    "        self.success_episode.append(\n",
    "            'Success' if win_or_lose == 'W' else 'Failure')\n",
    "\n",
    "        file = open('render/render.txt', 'a')\n",
    "        file.write('-------------------------------------------\\n')\n",
    "        file.write(f'Episode number {self.current_episode}\\n')\n",
    "        file.write(f'{self.success_episode[-1]} in {self.current_step} steps\\n')\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model output \"Tensor(\"dense_23/BiasAdd:0\", shape=(None, 5, 4), dtype=float32)\" has invalid shape. DQN expects a model that has one dimension for each action, in this case 4.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/7f/x9dv4lzx55lcn4rmfdb92mxr0000gn/T/ipykernel_3133/2113750407.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0mactions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction_space\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobservation_space\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mactions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m \u001B[0mdqn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_agent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mactions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m \u001B[0mdqn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mAdam\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'mae'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0mdqn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnb_steps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m50000\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvisualize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/7f/x9dv4lzx55lcn4rmfdb92mxr0000gn/T/ipykernel_3133/2113750407.py\u001B[0m in \u001B[0;36mbuild_agent\u001B[0;34m(model, actions)\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[0mpolicy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBoltzmannQPolicy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0mmemory\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSequentialMemory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlimit\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m50000\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwindow_length\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m     dqn = DQNAgent(model=model, memory=memory, policy=policy,\n\u001B[0m\u001B[1;32m     29\u001B[0m                    nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n\u001B[1;32m     30\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdqn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venv/lib/python3.8/site-packages/rl/agents/dqn.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001B[0m\n\u001B[1;32m    105\u001B[0m         \u001B[0;31m# Validate (important) input.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    106\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnb_actions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 107\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Model output \"{model.output}\" has invalid shape. DQN expects a model that has one dimension for each action, in this case {self.nb_actions}.'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    108\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0;31m# Parameters.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Model output \"Tensor(\"dense_23/BiasAdd:0\", shape=(None, 5, 4), dtype=float32)\" has invalid shape. DQN expects a model that has one dimension for each action, in this case 4."
     ]
    }
   ],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy  # MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO  # PPO2\n",
    "\n",
    "from env import MazeEnv\n",
    "\n",
    "# env = DummyVecEnv([lambda: MazeEnv(verbose = False)])\n",
    "# model = PPO(ActorCriticPolicy, env, learning_rate=0.001)\n",
    "# model.learn(10000)\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "env = MazeEnv()\n",
    "actions = env.action_space.n\n",
    "model = build_model(env.observation_space.shape, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}